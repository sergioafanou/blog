---
layout: post
title: How To Analyze Managed Redis Database Statistics Using the Elastic Stack on Ubuntu 18.04
network: digitalocean
date: September 26, 2019 at 04:01PM
url: https://www.digitalocean.com/community/tutorials/how-to-analyze-managed-redis-database-statistics-using-the-elastic-stack-on-ubuntu-18-04
image: https://assets.digitalocean.com/articles/redis_elk/step1.png
tags: docker
feedtitle: DigitalOcean Community Tutorials
feedurl: https://www.digitalocean.com/community/tutorials
author: DigitalOcean
---
<p><em>The author selected the <a href="https://www.brightfunds.org/funds/foss-nonprofits">Free and Open Source Fund</a> to receive a donation as part of the <a href="https://do.co/w4do-cta">Write for DOnations</a> program.</em></p>

<h3 id="introduction">Introduction</h3>

<p>Database monitoring is the continuous process of systematically tracking various metrics that show how the database is performing. By observing performance data, you can gain valuable insights and identify possible bottlenecks, as well as find additional ways of improving database performance. Such systems often implement alerting that notifies administrators when things go wrong. Gathered statistics can be used to not only improve the configuration and workflow of the database, but also those of client applications.</p>

<p>The benefit of using the <a href="https://www.elastic.co/products/elastic-stack">Elastic Stack</a> (ELK stack) for monitoring your managed database is its excellent support for searching and the ability to ingest new data very quickly. It does not excel at updating the data, but this trade-off is acceptable for monitoring and logging purposes, where past data is almost never changed. <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> offers a powerful means of querying the data, which you can use through <a href="https://www.elastic.co/products/kibana">Kibana</a> to get a better understanding of how the database fares through different time periods. This will allow you to correlate database load with real-life events to gain insight into how the database is being used.</p>

<p>In this tutorial, you&rsquo;ll import database metrics, generated by the Redis <a href="https://redis.io/commands/info">INFO</a> command, into Elasticsearch via <a href="https://www.elastic.co/products/logstash">Logstash</a>. This entails configuring Logstash to periodically run the command, parse its output and send it to Elasticsearch for indexing immediately afterward. The imported data can later be analyzed and visualized in Kibana. By the end of the tutorial, you&rsquo;ll have an automated system pulling in Redis statistics for later analysis.</p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
<li>An Ubuntu 18.04 server with at least 4 GB RAM, root privileges, and a secondary, non-root account. You can set this up by following <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-18-04">this initial server setup guide</a>. For this tutorial the non-root user is <code><span class="highlight">sammy</span></code>.</li>
<li>Java 8 installed on your server. For installation instructions, visit <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-18-04#installing-specific-versions-of-openjdk">How To Install Java with <code>apt</code> on Ubuntu 18.04</a>.</li>
<li>Nginx installed on your server. For a guide on how to do that, see <a href="https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-ubuntu-18-04">How To Install Nginx on Ubuntu 18.04</a>.</li>
<li>Elasticsearch and Kibana installed on your server. Complete the first two steps of the <a href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04">How To Install Elasticsearch, Logstash, and Kibana (Elastic Stack) on Ubuntu 18.04</a> tutorial.</li>
<li>A Redis managed database provisioned from DigitalOcean with connection information available. Make sure that your server&rsquo;s IP address is on the whitelist. To learn more about DigitalOcean Managed Databases, visit the <a href="https://www.digitalocean.com/docs/databases/overview/">product docs</a>.</li>
<li><a href="https://github.com/IBM-Cloud/redli">Redli</a> installed on your server according to the <a href="https://www.digitalocean.com/community/tutorials/how-to-connect-to-managed-database-ubuntu-18-04#connecting-to-a-managed-redis-database">How To Connect to a Managed Database on Ubuntu 18.04</a> tutorial.</li>
</ul>

<h2 id="step-1-—-installing-and-configuring-logstash">Step 1 — Installing and Configuring Logstash</h2>

<p>In this section, you will install Logstash and configure it to pull statistics from your Redis database cluster, then parse them to send to Elasticsearch for indexing.</p>

<p>Start off by installing Logstash with the following command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo apt install logstash -y
</li></ul></code></pre>
<p>Once Logstash is installed, enable the service to automatically start on boot:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl enable logstash
</li></ul></code></pre>
<p>Before configuring Logstash to pull the statistics, let&rsquo;s see what the data itself looks like. To connect to your Redis database, head over to your Managed Database Control Panel, and under the <strong>Connection details</strong> panel, select <strong>Flags</strong> from the dropdown:</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step1.png" alt="Managed Database Control Panel"></p>

<p>You&rsquo;ll be shown a preconfigured command for the <a href="https://github.com/IBM-Cloud/redli">Redli</a> client, which you&rsquo;ll use to connect to your database. Click <strong>Copy</strong> and run the following command on your server, replacing <code><span class="highlight">redli_flags_command</span></code> with the command you have just copied:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$"><span class="highlight">redli_flags_command</span> info
</li></ul></code></pre>
<p>Since the output from this command is long, we&rsquo;ll explain this broken down into its different sections:</p>

<p>In the output of the Redis <code>info</code> command, sections are marked with <code>#</code>, which signifies a comment. The values are populated in the form of <code>key:value</code>, which makes them relatively easy to parse.</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div># Server
redis_version:5.0.4
redis_git_sha1:ab60b2b1
redis_git_dirty:1
redis_build_id:7909f4de3561dc50
redis_mode:standalone
os:Linux 5.2.14-200.fc30.x86_64 x86_64
arch_bits:64
multiplexing_api:epoll
atomicvar_api:atomic-builtin
gcc_version:9.1.1
process_id:72
run_id:ddb7b96c93bbd0c369c6d06ce1c02c78902e13cc
tcp_port:25060
uptime_in_seconds:1733
uptime_in_days:0
hz:10
configured_hz:10
lru_clock:8687593
executable:/usr/bin/redis-server
config_file:/etc/redis.conf

# Clients
connected_clients:3
client_recent_max_input_buffer:2
client_recent_max_output_buffer:0
blocked_clients:0

. . .
</code></pre>
<p>The <code>Server</code> section contains technical information about the Redis build, such as its version and the Git commit it&rsquo;s based on. While the <code>Clients</code> section provides the number of currently opened connections.</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>. . .
# Memory
used_memory:941560
used_memory_human:919.49K
used_memory_rss:4931584
used_memory_rss_human:4.70M
used_memory_peak:941560
used_memory_peak_human:919.49K
used_memory_peak_perc:100.00%
used_memory_overhead:912190
used_memory_startup:795880
used_memory_dataset:29370
used_memory_dataset_perc:20.16%
allocator_allocated:949568
allocator_active:1269760
allocator_resident:3592192
total_system_memory:1030356992
total_system_memory_human:982.62M
used_memory_lua:37888
used_memory_lua_human:37.00K
used_memory_scripts:0
used_memory_scripts_human:0B
number_of_cached_scripts:0
maxmemory:463470592
maxmemory_human:442.00M
maxmemory_policy:allkeys-lru
allocator_frag_ratio:1.34
allocator_frag_bytes:320192
allocator_rss_ratio:2.83
allocator_rss_bytes:2322432
rss_overhead_ratio:1.37
rss_overhead_bytes:1339392
mem_fragmentation_ratio:5.89
mem_fragmentation_bytes:4093872
mem_not_counted_for_evict:0
mem_replication_backlog:0
mem_clients_slaves:0
mem_clients_normal:116310
mem_aof_buffer:0
mem_allocator:jemalloc-5.1.0
active_defrag_running:0
lazyfree_pending_objects:0
. . .
</code></pre>
<p>Here <code>Memory</code> confirms how much RAM Redis has allocated for itself, as well as the maximum amount of memory it can possibly use. If it starts running out of memory, it will free up keys using the strategy you specified in the Control Panel (shown in the <code>maxmemory_policy</code> field in this output).</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>. . .
# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1568966978
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
rdb_last_cow_size:217088
aof_enabled:0
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
aof_last_cow_size:0

# Stats
total_connections_received:213
total_commands_processed:2340
instantaneous_ops_per_sec:1
total_net_input_bytes:39205
total_net_output_bytes:776988
instantaneous_input_kbps:0.02
instantaneous_output_kbps:2.01
rejected_connections:0
sync_full:0
sync_partial_ok:0
sync_partial_err:0
expired_keys:0
expired_stale_perc:0.00
expired_time_cap_reached_count:0
evicted_keys:0
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:353
migrate_cached_sockets:0
slave_expires_tracked_keys:0
active_defrag_hits:0
active_defrag_misses:0
active_defrag_key_hits:0
active_defrag_key_misses:0
. . .
</code></pre>
<p>In the <code>Persistence</code> section, you can see the last time Redis saved the keys it stores to disk, and if it was successful. The <code>Stats</code> section provides numbers related to client and in-cluster connections, the number of times the requested key was (or wasn&rsquo;t) found, and so on.</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>. . .
# Replication
role:master
connected_slaves:0
master_replid:9c1d345a46d29d08537981c4fc44e312a21a160b
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:0
second_repl_offset:-1
repl_backlog_active:0
repl_backlog_size:46137344
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
. . .
</code></pre>
<p><span class='note'><strong>Note:</strong> The Redis project uses the terms &ldquo;master&rdquo; and &ldquo;slave&rdquo; in its documentation and in various commands. DigitalOcean generally prefers the alternative terms &ldquo;primary&rdquo; and &ldquo;replica.&rdquo;<br>
This guide will default to the terms &ldquo;primary&rdquo; and &ldquo;replica&rdquo; whenever possible, but note that there are a few instances where the terms &ldquo;master&rdquo; and &ldquo;slave&rdquo; unavoidably come up.<br></span></p>

<p>By looking at the <code>role</code> under <code>Replication</code>, you&rsquo;ll know if you&rsquo;re connected to a primary or replica node. The rest of the section provides the number of currently connected replicas and the amount of data that the replica is lacking in regards to the primary. There may be additional fields if the instance you are connected to is a replica.</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>. . .
# CPU
used_cpu_sys:1.972003
used_cpu_user:1.765318
used_cpu_sys_children:0.000000
used_cpu_user_children:0.001707

# Cluster
cluster_enabled:0

# Keyspace

</code></pre>
<p>Under <code>CPU</code>, you&rsquo;ll see the amount of system (<code>used_cpu_sys</code>) and user (<code>used_cpu_user</code>) CPU Redis is consuming at the moment. The <code>Cluster</code> section contains only one unique field, <code>cluster_enabled</code>, which serves to indicate that the Redis cluster is running.</p>

<p>Logstash will be tasked to periodically run the <code>info</code> command on your Redis database (similar to how you just did), parse the results, and send them to Elasticsearch. You&rsquo;ll then be able to access them later from Kibana.</p>

<p>You&rsquo;ll store the configuration for indexing Redis statistics in Elasticsearch in a file named <code>redis.conf</code> under the <code>/etc/logstash/conf.d</code> directory, where Logstash stores configuration files. When started as a service, it will automatically run them in the background.</p>

<p>Create <code>redis.conf</code> using your favorite editor (for example, nano):</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo nano /etc/logstash/conf.d/redis.conf
</li></ul></code></pre>
<p>Add the following lines:</p>
<div class="code-label " title="/etc/logstash/conf.d/redis.conf">/etc/logstash/conf.d/redis.conf</div><pre class="code-pre "><code langs="">input {
    exec {
        command =&gt; "<span class="highlight">redis_flags_command</span> info"
        interval =&gt; 10
        type =&gt; "redis_info"
    }
}

filter {
    kv {
        value_split =&gt; ":"
        field_split =&gt; "\r\n"
        remove_field =&gt; [ "command", "message" ]
    }

    ruby {
        code =&gt;
        "
        event.to_hash.keys.each { |k|
            if event.get(k).to_i.to_s == event.get(k) # is integer?
                event.set(k, event.get(k).to_i) # convert to integer
            end
            if event.get(k).to_f.to_s == event.get(k) # is float?
                event.set(k, event.get(k).to_f) # convert to float
            end
        }
        puts 'Ruby filter finished'
        "
    }
}

output {
    elasticsearch {
        hosts =&gt; "http://localhost:9200"
        index =&gt; "%{type}"
    }
}
</code></pre>
<p>Remember to replace <code><span class="highlight">redis_flags_command</span></code> with the command shown in the control panel that you used earlier in the step.</p>

<p>You define an <code>input</code>, which is a set of filters that will run on the collected data, and an output that will send the filtered data to Elasticsearch. The input consists of the <code>exec</code> command, which will run a <code>command</code> on the server periodically, after a set time <code>interval</code> (expressed in seconds). It also specifies a <code>type</code> parameter that defines the document type when indexed in Elasticsearch. The <code>exec</code> block passes down an object containing two fields, <code>command</code> and <code>message</code> string. The <code>command</code> field will contain the command that was run, and the <code>message</code> will contain its output.</p>

<p>There are two filters that will run sequentially on the data collected from the input. The <code>kv</code> filter stands for key-value filter, and is built-in to Logstash. It is used for parsing data in the general form of <code>key<span class="highlight">value_separator</span>value</code> and provides parameters for specifying what are considered a value and field separators. The field separator pertains to strings that separate the data formatted in the general form from each other. In the case of the output of the Redis INFO command, the field separator (<code>field_split</code>) is a new line, and the value separator (<code>value_split</code>) is <code>:</code>. Lines that do not follow the defined form will be discarded, including comments.</p>

<p>To configure the <code>kv</code> filter, you pass <code>:</code> to the<code>value_split</code> parameter, and <code>\r\n</code> (signifying a new line) to the <code>field_split</code> parameter. You also order it to remove the <code>command</code> and <code>message</code> fields from the current data object by passing them to <code>remove_field</code> as elements of an array, because they contain data that are now useless.</p>

<p>The <code>kv</code> filter represents the value it parsed as a string (text) type by design. This raises an issue because Kibana can&rsquo;t easily process string types, even if it&rsquo;s actually a number. To solve this, you&rsquo;ll use custom Ruby code to convert the number-only strings to numbers, where possible. The second filter is a <code>ruby</code> block that provides a <code>code</code> parameter accepting a string containing the code to be run.</p>

<p><code>event</code> is a variable that Logstash provides to your code, and contains the current data in the filter pipeline. As was noted before, filters run one after another, meaning that the Ruby filter will receive the parsed data from the <code>kv</code> filter. The Ruby code itself converts the <code>event</code> to a Hash and traverses through the keys, then checks if the value associated with the key could be represented as an integer or as a float (a number with decimals). If it can, the string value is replaced with the parsed number. When the loop finishes, it prints out a message (<code>Ruby filter finished</code>) to report progress.</p>

<p>The output sends the processed data to Elasticsearch for indexing. The resulting document will be stored in the <code>redis_info</code> index, defined in the input and passed in as a parameter to the output block.</p>

<p>Save and close the file.</p>

<p>You&rsquo;ve installed Logstash using <code>apt</code> and configured it to periodically request statistics from Redis, process them, and send them to your Elasticsearch instance.</p>

<h2 id="step-2-—-testing-the-logstash-configuration">Step 2 — Testing the Logstash Configuration</h2>

<p>Now you&rsquo;ll test the configuration by running Logstash to verify it will properly pull the data.</p>

<p>Logstash supports running a specific configuration by passing its file path to the <code>-f</code> parameter. Run the following command to test your new configuration from the last step:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo /usr/share/logstash/bin/logstash -f /etc/logstash/conf.d/redis.conf
</li></ul></code></pre>
<p>It may take some time to show the output, but you&rsquo;ll soon see something similar to the following:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults
Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console
[WARN ] 2019-09-20 11:59:53.440 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified
[INFO ] 2019-09-20 11:59:53.459 [LogStash::Runner] runner - Starting Logstash {"logstash.version"=&gt;"6.8.3"}
[INFO ] 2019-09-20 12:00:02.543 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Starting pipeline {:pipeline_id=&gt;"main", "pipeline.workers"=&gt;2, "pipeline.batch.size"=&gt;125, "pipeline.batch.delay"=&gt;50}
[INFO ] 2019-09-20 12:00:03.331 [[main]-pipeline-manager] elasticsearch - Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[http://localhost:9200/]}}
[WARN ] 2019-09-20 12:00:03.727 [[main]-pipeline-manager] elasticsearch - Restored connection to ES instance {:url=&gt;"http://localhost:9200/"}
[INFO ] 2019-09-20 12:00:04.015 [[main]-pipeline-manager] elasticsearch - ES Output version determined {:es_version=&gt;6}
[WARN ] 2019-09-20 12:00:04.020 [[main]-pipeline-manager] elasticsearch - Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=&gt;6}
[INFO ] 2019-09-20 12:00:04.071 [[main]-pipeline-manager] elasticsearch - New Elasticsearch output {:class=&gt;"LogStash::Outputs::ElasticSearch", :hosts=&gt;["http://localhost:9200"]}
[INFO ] 2019-09-20 12:00:04.100 [Ruby-0-Thread-5: :1] elasticsearch - Using default mapping template
[INFO ] 2019-09-20 12:00:04.146 [Ruby-0-Thread-5: :1] elasticsearch - Attempting to install template {:manage_template=&gt;{"template"=&gt;"logstash-*", "version"=&gt;60001, "settings"=&gt;{"index.refresh_interval"=&gt;"5s"}, "mappings"=&gt;{"_default_"=&gt;{"dynamic_templates"=&gt;[{"message_field"=&gt;{"path_match"=&gt;"message", "match_mapping_type"=&gt;"string", "mapping"=&gt;{"type"=&gt;"text", "norms"=&gt;false}}}, {"string_fields"=&gt;{"match"=&gt;"*", "match_mapping_type"=&gt;"string", "mapping"=&gt;{"type"=&gt;"text", "norms"=&gt;false, "fields"=&gt;{"keyword"=&gt;{"type"=&gt;"keyword", "ignore_above"=&gt;256}}}}}], "properties"=&gt;{"@timestamp"=&gt;{"type"=&gt;"date"}, "@version"=&gt;{"type"=&gt;"keyword"}, "geoip"=&gt;{"dynamic"=&gt;true, "properties"=&gt;{"ip"=&gt;{"type"=&gt;"ip"}, "location"=&gt;{"type"=&gt;"geo_point"}, "latitude"=&gt;{"type"=&gt;"half_float"}, "longitude"=&gt;{"type"=&gt;"half_float"}}}}}}}}
[INFO ] 2019-09-20 12:00:04.295 [[main]-pipeline-manager] exec - Registering Exec Input {:type=&gt;"redis_info", :command=&gt;"...", :interval=&gt;10, :schedule=&gt;nil}
[INFO ] 2019-09-20 12:00:04.315 [Converge PipelineAction::Create&lt;main&gt;] pipeline - Pipeline started successfully {:pipeline_id=&gt;"main", :thread=&gt;"#&lt;Thread:0x73adceba run&gt;"}
[INFO ] 2019-09-20 12:00:04.483 [Ruby-0-Thread-1: /usr/share/logstash/lib/bootstrap/environment.rb:6] agent - Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}
[INFO ] 2019-09-20 12:00:05.318 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=&gt;9600}
Ruby filter finished
Ruby filter finished
Ruby filter finished
...
</code></pre>
<p>You&rsquo;ll see the <code>Ruby filter finished</code> message being printed at regular intervals (set to 10 seconds in the previous step), which means that the statistics are being shipped to Elasticsearch.</p>

<p>You can exit Logstash by clicking <code>CTRL + C</code> on your keyboard. As previously mentioned, Logstash will automatically run all config files found under <code>/etc/logstash/conf.d</code> in the background when started as a service. Run the following command to start it:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl start logstash
</li></ul></code></pre>
<p>You&rsquo;ve run Logstash to check if it can connect to your Redis cluster and gather data. Next, you&rsquo;ll explore some of the statistical data in Kibana.</p>

<h2 id="step-3-—-exploring-imported-data-in-kibana">Step 3 — Exploring Imported Data in Kibana</h2>

<p>In this section, you&rsquo;ll explore and visualize the statistical data describing your database&rsquo;s performance in Kibana.</p>

<p>In your web browser, navigate to your domain where you exposed Kibana as a part of the prerequisite. You&rsquo;ll see the default welcome page:</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3a.png" alt="Kibana - Welcome Page"></p>

<p>Before exploring the data Logstash is sending to Elasticsearch, you&rsquo;ll first need to add the <code>redis_info</code> index to Kibana. To do so, click on <strong>Management</strong> from the left-hand vertical sidebar, and then on <strong>Index Patterns</strong> under the <strong>Kibana</strong> section.</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3b.png" alt="Kibana - Index Pattern Creation"></p>

<p>You&rsquo;ll see a form for creating a new <strong>Index Pattern</strong>. Index Patterns in Kibana provide a way to pull in data from multiple Elasticsearch indexes at once, and can be used to explore only one index.</p>

<p>Beneath the <strong>Index pattern</strong> text field, you&rsquo;ll see the <code>redis_info</code> index listed. Type it in the text field and then click on the <strong>Next step</strong> button.</p>

<p>You&rsquo;ll then be asked to choose a timestamp field, so you&rsquo;ll later be able to narrow your searches by a time range. Logstash automatically adds one, called <code>@timestamp</code>. Select it from the dropdown and click on <strong>Create index pattern</strong> to finish adding the index to Kibana.</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3c.png" alt="Kibana - Index Pattern Timestamp Selection"></p>

<p>To create and see existing visualizations, click on the <strong>Visualize</strong> item in the left-hand vertical menu. You&rsquo;ll see the following page:</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3d.png" alt="Kibana - Visualizations"></p>

<p>To create a new visualization, click on the <strong>Create a visualization</strong> button, then select <strong>Line</strong> from the list of types that will pop up. Then, select the <code>redis_info*</code> index pattern you have just created as the data source. You&rsquo;ll see an empty visualization:</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3e.png" alt="Kibana - Empty Visualization"></p>

<p>The left-side panel provides a form for editing parameters that Kibana will use to draw the visualization, which will be shown on the central part of the screen. On the upper-right hand side of the screen is the date range picker. If the <code>@timestamp</code> field is being used in the visualization, Kibana will only show the data belonging to the time interval specified in the range picker.</p>

<p>You’ll now visualize the average Redis memory usage during a specified time interval. Click on <strong>Y-Axis</strong> under <strong>Metrics</strong> in the panel on the left to unfold it, then select <em>Average</em> as the <strong>Aggregation</strong> and select <code>used_memory</code> as the <strong>Field</strong>. This will populate the Y axis of the plot with the average values.</p>

<p>Next, click on <strong>X-Axis</strong> under <strong>Buckets</strong>. For the <strong>Aggregation</strong>, choose <strong>Date Histogram</strong>. <code>@timestamp</code> should be automatically selected as the <strong>Field</strong>. Then, show the visualization by clicking on the blue play button on the top of the panel. If your database is brand new and not used you won’t see a very long line. In all cases, however, you will see an accurate portrayal of average memory usage. Here is how the resulting visualization may look after little to no usage:</p>

<p><img src="https://assets.digitalocean.com/articles/redis_elk/step3f.png" alt="Kibana - Redis Memory Usage Visualization"></p>

<p>In this step, you have visualized memory usage of your managed Redis database, using Kibana. You can also use other plot types Kibana offers, such as the Visual Builder, to create more complicated graphs that portray more than one field at the same time. This will allow you to gain a better understanding of how your database is being used, which will help you optimize client applications, as well as your database itself.</p>

<h2 id="conclusion">Conclusion</h2>

<p>You now have the Elastic stack installed on your server and configured to pull statistics data from your managed Redis database on a regular basis. You can analyze and visualize the data using Kibana, or some other suitable software, which will help you gather valuable insights and real-world correlations into how your database is performing.</p>

<p>For more information about what you can do with your Redis Managed Database, visit the <a href="https://www.digitalocean.com/docs/databases/redis/">product docs</a>. If you&rsquo;d like to present the database statistics using another visualization type, check out the <a href="https://www.elastic.co/guide/en/kibana/current/tutorial-visualizing.html">Kibana docs</a> for further instructions.</p>
