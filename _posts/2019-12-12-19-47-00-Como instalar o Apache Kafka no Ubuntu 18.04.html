---
layout: post
title: Como instalar o Apache Kafka no Ubuntu 18.04
network: digitalocean
date: December 12, 2019 at 07:47PM
url: https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-ubuntu-18-04-pt
image: http://ifttt.com/images/no_image_card.png
tags: docker
feedtitle: DigitalOcean Community Tutorials
feedurl: https://www.digitalocean.com/community/tutorials
author: DigitalOcean
---
<p><em>O autor selecionou o <a href="https://www.brightfunds.org/funds/foss-nonprofits">Free and Open Source Fund</a> para receber uma doação como parte do programa <a href="https://do.co/w4do-cta">Write for DOnations</a>.</em></p>

<h3 id="introdução">Introdução</h3>

<p><a href="http://kafka.apache.org/">O Apache Kafka</a> é um message broker popularmente distribuído projetado para lidar de forma eficiente com grandes volumes de dados em tempo real. Um cluster Kafka é não só altamente escalável e tolerante a falhas, mas ele também tem uma taxa de transferência muito mais alta comparada com outros message brokers como o <a href="http://activemq.apache.org/">ActiveMQ</a> e o <a href="https://www.rabbitmq.com/">RabbitMQ</a>. Embora ele seja geralmente usado como um sistema de mensagens <em>publicar/assinar</em>, muitas organizações também o usam para a agregação de registros porque ele oferece armazenamento persistente para mensagens publicadas.</p>

<p>Um sistema de mensagens publicar/assinar permite que um ou mais produtores publiquem mensagens sem considerar o número de consumidores ou como irão processar as mensagens. Clientes assinantes são notificados automaticamente sobre atualizações e a criação de novas mensagens. Este sistema é mais eficiente e escalável do que sistemas onde clientes questionam periodicamente para determinar se novas mensagens estão disponíveis.</p>

<p>Neste tutorial, você instalará e usará o Apache Kafka 2.1.1 no Ubuntu 18.04.</p>

<h2 id="pré-requisitos">Pré-requisitos</h2>

<p>Para acompanhar, você precisará de:</p>

<ul>
<li>Um servidor Ubuntu 18.04 e um usuário que não seja o root com privilégios sudo. Siga os passos especificados neste <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-18-04">guia</a> se não tiver um usuário que não seja root configurado.</li>
<li>Pelo menos 4GB de RAM no servidor. As instalações sem essa quantidade de RAM podem fazer com que o serviço Kafka falhe, com a máquina virtual <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java (JVM)</a> que lança uma exceção &ldquo;Fora de Memória&rdquo; durante a inicialização.</li>
<li><a href="http://openjdk.java.net/">OpenJDK</a> 8 instalado no seu servidor. Para instalar essa versão, siga <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-ubuntu-18-04#installing-specific-versions-of-openjdk">essas instruções</a> sobre a instalação de versões específicas do OpenJDK. O Kafka está escrito em Java, de modo que ele exige um JVM; no entanto, o script de inicialização shell possui um erro de detecção de versão que faz com que ele não comece com versões JVM acima de 8.</li>
</ul>

<h2 id="passo-1-—-criando-um-usuário-para-o-kafka">Passo 1 — Criando um Usuário para o Kafka</h2>

<p>Uma vez que o Kafka pode lidar com pedidos em uma rede, você deve criar um usuário dedicado para ele. Isso minimiza os danos na sua máquina Ubuntu caso o servidor Kafka seja comprometido. Criaremos um usuário *<em>kafka *</em>dedicado neste passo, mas você deve criar um usuário que não seja root para executar outras tarefas neste servidor uma vez que você tenha terminado de configurar o Kafka.</p>

<p>Logado como usuário sudo que não seja root, crie um usuário chamado <strong>kafka</strong> com o comando <code>useradd</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo useradd kafka -m
</li></ul></code></pre>
<p>A flag <code>-m</code> garante que um diretório home seja criada para o usuário. Este diretório home, <code>/home/kafka</code>, agirá como nossa pasta de trabalho para executar comandos nas seções abaixo.</p>

<p>Defina a senha usando <code>passwd</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka
</li></ul></code></pre>
<p>Adicione o usuário *<em>kafka *</em>ao grupo <code>sudo</code> com o comando <code>adduser</code>, de modo que ele tenha os privilégios necessários para instalar as dependências do Kafka:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo adduser kafka sudo
</li></ul></code></pre>
<p>Seu usuário** kafka** agora está pronto. Logue nesta conta usando <code>su</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">su -l kafka
</li></ul></code></pre>
<p>Agora que criamos o usuário específico do Kafka, podemos seguir para o download e extração dos binários Kafka.</p>

<h2 id="passo-2-—-fazendo-download-e-extraindo-os-binários-kafka">Passo 2 — Fazendo download e Extraindo os Binários Kafka</h2>

<p>Vamos baixar e extrair os binários Kafka em pastas dedicadas no diretório home do nosso usuário <strong>kafka</strong>.</p>

<p>Para começar, crie uma pasta em <code>/home/kafka</code> chamada <code>Downloads</code> para armazenar os seus downloads:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">mkdir ~/Downloads
</li></ul></code></pre>
<p>Use <code>curl</code> para baixar os binários Kafka:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl "https://www.apache.org/dist/kafka/<span class="highlight">2.1.1</span>/kafka_<span class="highlight">2.11-2.1.1</span>.tgz" -o ~/Downloads/kafka.tgz
</li></ul></code></pre>
<p>Crie um diretório chamado <code>kafka</code> e mude para este diretório. Este é o diretório base da instalação do Kafka:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">mkdir ~/kafka &amp;&amp; cd ~/kafka
</li></ul></code></pre>
<p>Extraia o arquivo que você baixou usando o comando <code>tar</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">tar -xvzf ~/Downloads/kafka.tgz --strip 1
</li></ul></code></pre>
<p>Especificamos a flag <code>--strip 1</code> para garantir que o conteúdo do arquivo seja extraído efetivamente em <code>~/kafka/</code> e não em outro diretório (como <code>~/kafka/kafka_<span class="highlight">2.11-2.1.1</span>/</code>) dentro dele.</p>

<p>Agora que baixamos e extraímos os binários com sucesso, podemos começar a configurar para que o Kafka permita que deletemos tópicos.</p>

<h2 id="passo-3-—-configurando-o-servidor-do-kafka">Passo 3 — Configurando o Servidor do Kafka</h2>

<p>O comportamento padrão do Kafka não nos permitirá excluir um um_ tópico_, a categoria, grupo ou nome do feed para os quais mensagens podem ser publicadas. Para modificar isso, vamos editar o arquivo de configuração.</p>

<p>As opções de configuração do Kafka são especificadas em <code>server.properties</code>. Abra este arquivo com o <code>nano</code> ou seu editor favorito:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">nano ~/kafka/config/server.properties
</li></ul></code></pre>
<p>Vamos adicionar um valor que nos permitirá excluir tópicos do Kafka. Adicione o seguinte ao final do arquivo:</p>
<div class="code-label " title="~/kafka/config/server.properties">~/kafka/config/server.properties</div><pre class="code-pre "><code langs="">delete.topic.enable = true
</code></pre>
<p>Salve o arquivo e saia do <code>nano</code>. Agora que configuramos o Kafka, podemos seguir para a criação de arquivos de unidade systemd para a executá-lo e permiti-lo na inicialização.</p>

<h2 id="passo-4-—-criando-arquivos-de-unidade-systemd-e-iniciando-o-servidor-do-kafka">Passo 4 — Criando Arquivos de Unidade Systemd e Iniciando o Servidor do Kafka</h2>

<p>Nesta seção, criaremos <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files">arquivos de unidade systemd</a> para o serviço do Kafka. Isso nos ajudará a realizar ações de serviço comuns como iniciar, parar e reiniciar o Kafka de uma maneira consistente com outros serviços do Linux.</p>

<p>O Zookeeper é um serviço que o Kafka usa para gerenciar seu estado de cluster e configurações. É geralmente usado em muitos sistemas distribuídos como um componente integral. Se você quer saber mais sobre isso, visite os <a href="https://zookeeper.apache.org/doc/current/index.html">documentos Zookeeper oficiais</a>.</p>

<p>Crie o arquivo de unidade para o <code>zookeeper</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo nano /etc/systemd/system/zookeeper.service
</li></ul></code></pre>
<p>Digite a seguinte definição de unidade no arquivo:</p>
<div class="code-label " title="/etc/systemd/system/zookeeper.service">/etc/systemd/system/zookeeper.service</div><pre class="code-pre "><code langs="">[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>
<p>A seção <code>[Unit]</code> especifica que o Zookeeper exige conexão em rede e que o sistema de arquivos esteja pronto antes de começar.</p>

<p>A seção <code>[Service]</code> especifica que o systemd deve usar os arquivos de shell <code>zookeeper-server-start.sh</code> e <code>zookeeper-server-start.sh</code> para começar e parar o serviço. Ele também especifica que o Zookeeper deve ser reiniciado automaticamente se ele fechar inexplicavelmente.</p>

<p>A seguir, crie o arquivo de serviço systemd para o <code>kafka</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo nano /etc/systemd/system/kafka.service
</li></ul></code></pre>
<p>Digite a seguinte definição de unidade no arquivo:</p>
<div class="code-label " title="/etc/systemd/system/kafka.service">/etc/systemd/system/kafka.service</div><pre class="code-pre "><code langs="">[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties &gt; /home/kafka/kafka/kafka.log 2&gt;&amp;1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>
<p>A seção <code>[Unit]</code> especifica que este arquivo de unidade depende do <code>zookeeper.service</code>. Isso irá garantir que o <code>zookeeper</code> seja iniciado automaticamente quando o serviço <code>kafka</code> começar.</p>

<p>A seção <code>[Service]</code> especifica que o systemd deve usar os arquivos de shell <code>kafka-server-start.sh</code> e <code>kafka-server-stop.sh</code> para começar e parar o serviço. Ele também especifica que o Zookeeper deve ser reiniciado automaticamente se ele fechar inexplicavelmente.</p>

<p>Agora que as unidades foram definidas, inicie o Kafka com o comando a seguir:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl start kafka
</li></ul></code></pre>
<p>Para garantir que o servidor inicializou com sucesso, verifique os registros de diário para a unidade <code>kafka</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo journalctl -u kafka
</li></ul></code></pre>
<p>Você deve ver um resultado similar ao seguinte:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Jul 17 18:38:59 kafka-ubuntu systemd[1]: Started kafka.service.
</code></pre>
<p>Agora, você tem um servidor do Kafka escutando na porta <code>9092</code>.</p>

<p>Enquanto inicializávamos o serviço <code>kafka</code>, se nós pudéssemos reiniciar nosso servidor, ele não seria iniciado automaticamente. Para ativar o <code>kafka</code> na inicialização do servidor, execute:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl enable kafka
</li></ul></code></pre>
<p>Agora que começamos e ativamos os serviços, vamos verificar a instalação.</p>

<h2 id="passo-5-—-testando-a-instalação">Passo 5 — Testando a Instalação</h2>

<p>Vamos publicar e consumir uma mensagem <strong>“Hello World”</strong> para garantir que o servidor do Kafka está se comportando corretamente. Publicando mensagens no Kafka exige:</p>

<ul>
<li>Um <em>produtor</em> que permite a publicação de registros e dados em tópicos.</li>
<li>Um <em>consumidor</em> que lê mensagens e dados de tópicos.</li>
</ul>

<p>Primeiro, crie um tópico chamado <code>TutorialTopic</code> digitando:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic
</li></ul></code></pre>
<p>Você pode criar um produtor a partir da linha de comando usando o script <code>kafka-console-producer.sh</code>. Ele recebe o nome do host do servidor do Kafka, porta e um nome de tópico como argumentos.</p>

<p>Publique o string <code>"Hello, World"</code> no tópico <code>TutorialTopic</code> digitando:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic &gt; /dev/null
</li></ul></code></pre>
<p>A seguir, você pode criar um consumidor no Kafka usando o script <code>kafka-console-consumer.sh</code>. Ele recebe o nome do host e porta do servidor ZooKeeper, junto com um nome de tópico como argumentos.</p>

<p>O comando a seguir consome mensagens do <code>TutorialTopic</code>. Observe o uso da flag <code>--from-beginning</code> que permite o consumo de mensagens que foram publicadas antes do início do consumidor:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">~/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic TutorialTopic --from-beginning
</li></ul></code></pre>
<p>Se não houver problemas de configuração, você deve ver <code>Hello, World</code> no seu terminal:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Hello, World
</code></pre>
<p>O script continuará a executar, esperando que mais mensagens sejam publicadas no tópico. Sinta-se à vontade para abrir um novo terminal e iniciar um produtor para publicar mais algumas mensagens. Você deve poder ver todas elas na saída do consumidor.</p>

<p>Quando você acabar os testes, pressione <code>CTRL+C</code> para parar o script do consumidor. Agora que testamos a instalação, vamos prosseguir para a instalação do KafkaT.</p>

<h2 id="passo-6-—-instalar-o-kafkat-opcional">Passo 6 — Instalar o KafkaT (Opcional)</h2>

<p>O <a href="https://github.com/airbnb/kafkat">KafkaT</a> é uma ferramenta do Airbnb que torna mais fácil para você ver detalhes sobre seu cluster do Kafka e executar certas tarefas administrativas da linha de comando. Uma vez que é uma gem Ruby, você precisará do Ruby para usá-la. Você também precisará do pacote <code>build-essential</code> para poder construir outras gems das quais ele depende. Instale-os usando o <code>apt</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo apt install ruby ruby-dev build-essential
</li></ul></code></pre>
<p>Agora, você pode instalar o KafkaT usando o comando gem:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo gem install kafkat
</li></ul></code></pre>
<p>O KafkaT usa <code>.kafkatcfg</code> como o arquivo de configuração para determinar os diretórios e registro da instalação do seu servidor do Kafka. Ele também deve ter uma entrada apontando o KafkaT para a sua instância ZooKeeper.</p>

<p>Crie um arquivo novo chamado <code>.kafkatcfg</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">nano ~/.kafkatcfg
</li></ul></code></pre>
<p>Adicione as linhas a seguir para especificar as informações necessárias sobre o seu servidor do Kafka e a instância do Zookeeper:</p>
<div class="code-label " title="~/.kafkatcfg">~/.kafkatcfg</div><pre class="code-pre "><code langs="">{
  "kafka_path": "~/kafka",
  "log_path": "/tmp/kafka-logs",
  "zk_path": "localhost:2181"
}
</code></pre>
<p>Agora, você está pronto para usar o KafkaT. Para um começo, veja como você usaria ele para ver os detalhes sobre todas as partições do Kafka:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">kafkat partitions
</li></ul></code></pre>
<p>Você verá o seguinte resultado:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Topic                 Partition   Leader      Replicas        ISRs    
TutorialTopic         0             0         [0]             [0]
__consumer_offsets    0               0           [0]                           [0]
...
...
</code></pre>
<p>Você verá o <code>TutorialTopic</code>, além de <code>__consumer_offsets</code>, um tópico interno usado pelo Kafka para armazenar informações relacionadas ao cliente. Você pode ignorar com segurança linhas começando com <code>__consumer_offsets</code>.</p>

<p>Para aprender mais sobre o KafkaT, consulte o seu <a href="https://github.com/airbnb/kafkat">repositório do GitHub</a>.</p>

<h2 id="passo-7-—-configurando-um-cluster-multi-nodal-opcional">Passo 7 — Configurando um Cluster Multi-Nodal (Opcional)</h2>

<p>Se você quer criar um cluster multi-broker usando mais máquinas Ubuntu 18.04, você deve repetir o passo 1, passo 4, e o passo 5 em cada uma das novas máquinas. Além disso, você deve fazer as seguintes alterações no arquivo <code>server.properties</code> para cada um deles:</p>

<ul>
<li><p>O valor da propriedade <code>broker.id</code> deve ser alterado de modo que ele seja único ao longo do cluster. Esta propriedade identifica separadamente cada servidor no cluster e pode ter qualquer string como seu valor. Por exemplo, <code>"server1"</code>, <code>"server2"</code>, etc.</p></li>
<li><p>O valor da propriedade <code>zookeeper.connect</code> deve ser alterado de modo que todos os nós apontem para a mesma instância do ZooKeeper. Esta propriedade especifica o endereço da instância do Zookeeper e segue o formato <code>:&lt;PORT&gt;</code>. Por exemplo, <code>"<span class="highlight">203.0.113.0</span>:2181"</code>, <code>"<span class="highlight">203.0.113.1</span>:2181"</code> etc.</p></li>
</ul>

<p>Se você quer ter várias instâncias do ZooKeeper para o seu cluster, o valor da propriedade <code>zookeeper.connect</code> em cada nó deve ser uma string idêntica e separada por vírgulas que listem os endereços de IP e os números de porta de todas as instâncias do ZooKeeper.</p>

<h2 id="passo-8-—-restringindo-o-utilizador-do-kafka">Passo 8 — Restringindo o Utilizador do Kafka</h2>

<p>Agora que todas as instalações estão prontas, você pode remover os privilégios de admin do usuário <strong>kafka</strong>. Antes de fazer isso, saia e logue novamente como se fosse qualquer outro usuário sudo que não seja root. Se você ainda estiver executando a mesma sessão de shell com a qual você tenha iniciado este tutorial, simplesmente digite <code>exit</code>.</p>

<p>Remova o usuário <strong>kafka</strong> do grupo sudo:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo deluser kafka sudo
</li></ul></code></pre>
<p>Para melhorar ainda mais a segurança do seu servidor Kafka, trave a senha do usuário <strong>kafka</strong> usando o comando <code>passwd</code>. Isso faz com que ninguém possa logar diretamente no servidor usando essa conta:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka -l
</li></ul></code></pre>
<p>Neste ponto, apenas root ou um usuário sudo pode logar como <code>kafka</code> digitando o comando a seguir:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo su - kafka
</li></ul></code></pre>
<p>No futuro, se você quer destravá-lo, use <code>passwd</code> com a opção <code>-u</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka -u
</li></ul></code></pre>
<p>Agora, você restringiu com sucesso os privilégios de admin do usuário <strong>kafka</strong>.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Agora, você tem o Apache Kafka funcionando com segurança no seu servidor Ubuntu. Você pode usar isso nos seus projetos criando produtores do Kafka e consumidores usando <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">clientes do Kafka</a>, que estão disponíveis para a maioria das linguagens de programação. Para saber mais sobre o Kafka, você também pode consultar a sua <a href="http://kafka.apache.org/documentation.html">documentação</a>.</p>
