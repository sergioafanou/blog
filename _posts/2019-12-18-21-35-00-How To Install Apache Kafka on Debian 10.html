---
layout: post
title: How To Install Apache Kafka on Debian 10
network: digitalocean
date: December 18, 2019 at 09:35PM
url: https://www.digitalocean.com/community/tutorials/how-to-install-apache-kafka-on-debian-10
image: http://ifttt.com/images/no_image_card.png
tags: docker
feedtitle: DigitalOcean Community Tutorials
feedurl: https://www.digitalocean.com/community/tutorials
author: DigitalOcean
---
<p><em>The author selected the <a href="https://www.brightfunds.org/funds/foss-nonprofits">Free and Open Source Fund</a> to receive a donation as part of the <a href="https://do.co/w4do-cta">Write for DOnations</a> program.</em></p>

<h3 id="introduction">Introduction</h3>

<p><a href="http://kafka.apache.org/">Apache Kafka</a> is a popular distributed message broker designed to handle large volumes of real-time data. A Kafka cluster is highly scalable and fault-tolerant, and also has a much higher throughput compared to other message brokers such as <a href="http://activemq.apache.org/">ActiveMQ</a> and <a href="https://www.rabbitmq.com/">RabbitMQ</a>. Though it is generally used as a <em>publish/subscribe</em> messaging system, a lot of organizations also use it for log aggregation because it offers persistent storage for published messages.</p>

<p>A publish/subscribe messaging system allows one or more producers to publish messages without considering the number of consumers or how they will process the messages. Subscribed clients are notified automatically about updates and the creation of new messages. This system is more efficient and scalable than systems where clients poll periodically to determine if new messages are available.</p>

<p>In this tutorial, you will install and configure Apache Kafka 2.1.1 securely on a Debian 10 server, then test your setup by producing and consuming a <code>Hello World</code> message. You will then optionally install <a href="https://github.com/airbnb/kafkat">KafkaT</a> to monitor Kafka and set up a Kafka multi-node cluster. </p>

<h2 id="prerequisites">Prerequisites</h2>

<p>To follow along, you will need:</p>

<ul>
<li>One Debian 10 server with at least 4GB of RAM and a non-root user with sudo privileges. Follow the steps specified in our <a href="https://www.digitalocean.com/community/tutorials/initial-server-setup-with-debian-10">Initial Server Setup guide for Debian 10</a> if you do not have a non-root user set up.</li>
<li><a href="http://openjdk.java.net/">OpenJDK</a> 11 installed on your server. To install this version, follow the instructions in <a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-on-debian-10#installing-the-default-jrejdk">How To Install Java with Apt on Debian 10</a> on installing specific versions of OpenJDK. Kafka is written in Java, so it requires a JVM.</li>
</ul>

<p><span class='note'><strong>Note:</strong> Installations without 4GB of RAM may cause the Kafka service to fail, with the <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java virtual machine (JVM)</a> throwing an <code>Out Of Memory</code> exception during startup.<br></span></p>

<h2 id="step-1-—-creating-a-user-for-kafka">Step 1 — Creating a User for Kafka</h2>

<p>Since Kafka can handle requests over a network, it is a best practice to create a dedicated user for it. This minimizes damage to your Debian machine should the Kafka server be compromised. You will create the dedicated user <strong>kafka</strong> in this step.</p>

<p>Logged in as your non-root sudo user, create a user called <code>kafka</code> with the <code>useradd</code> command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo useradd kafka -m
</li></ul></code></pre>
<p>The <code>-m</code> flag ensures that a home directory will be created for the user. This home directory, <code>/home/kafka</code>, will act as your workspace directory for executing commands later on.</p>

<p>Set the password using <code>passwd</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka
</li></ul></code></pre>
<p>Enter the password you wish to use for this user.</p>

<p>Next, add the <strong>kafka</strong> user to the <code>sudo</code> group with the <code>adduser</code> command, so that it has the privileges required to install Kafka&rsquo;s dependencies:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo adduser kafka sudo
</li></ul></code></pre>
<p>Your <strong>kafka</strong> user is now ready. Log into this account using <code>su</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">su -l kafka
</li></ul></code></pre>
<p>Now that you&rsquo;ve created the Kafka-specific user, you can move on to downloading and extracting the Kafka binaries.</p>

<h2 id="step-2-—-downloading-and-extracting-the-kafka-binaries">Step 2 — Downloading and Extracting the Kafka Binaries</h2>

<p>In this step, you will download and extract the Kafka binaries into dedicated folders in your <strong>kafka</strong> user&rsquo;s home directory.</p>

<p>To start, create a directory in <code>/home/kafka</code> called <code>Downloads</code> to store your downloads:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">mkdir ~/Downloads
</li></ul></code></pre>
<p>Next, install <code>curl</code> using <code>apt-get</code> so that you&rsquo;ll be able to download remote files:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo apt-get update &amp;&amp; sudo apt-get install curl
</li></ul></code></pre>
<p>When prompted, type <code>Y</code> to confirm the <code>curl</code> download.</p>

<p>Once <code>curl</code> is installed, use it to download the Kafka binaries:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl "https://archive.apache.org/dist/kafka/<span class="highlight">2.1.1</span>/kafka_<span class="highlight">2.11-2.1.1</span>.tgz" -o ~/Downloads/kafka.tgz
</li></ul></code></pre>
<p>Create a directory called <code>kafka</code> and change to this directory. This will be the base directory of the Kafka installation:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">mkdir ~/kafka &amp;&amp; cd ~/kafka
</li></ul></code></pre>
<p>Extract the archive you downloaded using the <code>tar</code> command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">tar -xvzf ~/Downloads/kafka.tgz --strip 1
</li></ul></code></pre>
<p>You specified the <code>--strip 1</code> flag to ensure that the archive&rsquo;s contents are extracted in <code>~/kafka/</code> itself and not in another directory inside of it, such as <code>~/kafka/kafka_<span class="highlight">2.12-2.1.1</span>/</code>.</p>

<p>Now that you&rsquo;ve downloaded and extracted the binaries successfully, you can move on to configuring Kafka to allow for topic deletion.</p>

<h2 id="step-3-—-configuring-the-kafka-server">Step 3 — Configuring the Kafka Server</h2>

<p>Kafka&rsquo;s default behavior will not allow us to delete a <em>topic</em>, the category, group, or feed name to which messages can be published. To modify this, you will edit the configuration file.</p>

<p>Kafka&rsquo;s configuration options are specified in <code>server.properties</code>. Open this file with <code>nano</code> or your favorite editor:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">nano ~/kafka/config/server.properties
</li></ul></code></pre>
<p>Let&rsquo;s add a setting that will allow us to delete Kafka topics. Add the following highlighted line to the bottom of the file:</p>
<div class="code-label " title="~/kafka/config/server.properties">~/kafka/config/server.properties</div><pre class="code-pre "><code langs="">...
group.initial.rebalance.delay.ms

<span class="highlight">delete.topic.enable = true</span>
</code></pre>
<p>Save the file, and exit <code>nano</code>. Now that you&rsquo;ve configured Kafka, you can create <code>systemd</code> unit files for running and enabling Kafka on startup.</p>

<h2 id="step-4-—-creating-systemd-unit-files-and-starting-the-kafka-server">Step 4 — Creating Systemd Unit Files and Starting the Kafka Server</h2>

<p>In this section, you will create <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files"><code>systemd</code> unit files</a> for the Kafka service. This will help you perform common service actions such as starting, stopping, and restarting Kafka in a manner consistent with other Linux services.</p>

<p>ZooKeeper is a service that Kafka uses to manage its cluster state and configurations. It is commonly used in distributed systems as an integral component. In this tutorial, you will use Zookeeper to manage these aspects of Kafka. If you would like to know more about it, visit the official <a href="https://zookeeper.apache.org/doc/current/index.html">ZooKeeper docs</a>.</p>

<p>First, create the unit file for <code>zookeeper</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo nano /etc/systemd/system/zookeeper.service
</li></ul></code></pre>
<p>Enter the following unit definition into the file:</p>
<div class="code-label " title="/etc/systemd/system/zookeeper.service">/etc/systemd/system/zookeeper.service</div><pre class="code-pre "><code langs="">[Unit]
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
User=kafka
ExecStart=/home/kafka/kafka/bin/zookeeper-server-start.sh /home/kafka/kafka/config/zookeeper.properties
ExecStop=/home/kafka/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>
<p>The <code>[Unit]</code> section specifies that ZooKeeper requires networking and for the filesystem to be ready before it can start.</p>

<p>The <code>[Service]</code> section specifies that <code>systemd</code> should use the <code>zookeeper-server-start.sh</code> and <code>zookeeper-server-stop.sh</code> shell files for starting and stopping the service. It also specifies that ZooKeeper should be restarted automatically if it exits abnormally.</p>

<p>Next, create the <code>systemd</code> service file for <code>kafka</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo nano /etc/systemd/system/kafka.service
</li></ul></code></pre>
<p>Enter the following unit definition into the file:</p>
<div class="code-label " title="/etc/systemd/system/kafka.service">/etc/systemd/system/kafka.service</div><pre class="code-pre "><code langs="">[Unit]
Requires=zookeeper.service
After=zookeeper.service

[Service]
Type=simple
User=kafka
ExecStart=/bin/sh -c '/home/kafka/kafka/bin/kafka-server-start.sh /home/kafka/kafka/config/server.properties &gt; /home/kafka/kafka/kafka.log 2&gt;&amp;1'
ExecStop=/home/kafka/kafka/bin/kafka-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
</code></pre>
<p>The <code>[Unit]</code> section specifies that this unit file depends on <code>zookeeper.service</code>. This will ensure that <code>zookeeper</code> gets started automatically when the <code>kafka</code> service starts.</p>

<p>The <code>[Service]</code> section specifies that <code>systemd</code> should use the <code>kafka-server-start.sh</code> and <code>kafka-server-stop.sh</code> shell files for starting and stopping the service. It also specifies that Kafka should be restarted automatically if it exits abnormally.</p>

<p>Now that the units have been defined, start Kafka with the following command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl start kafka
</li></ul></code></pre>
<p>To ensure that the server has started successfully, check the journal logs for the <code>kafka</code> unit:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo journalctl -u kafka
</li></ul></code></pre>
<p>You will see output similar to the following:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Mar 23 13:31:48 kafka systemd[1]: Started kafka.service.
</code></pre>
<p>You now have a Kafka server listening on port <code>9092</code>, which is the default port for Kafka.</p>

<p>You have started the <code>kafka</code> service, but if you were to reboot your server, it would not yet be started automatically. To enable <code>kafka</code> on server boot, run:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo systemctl enable kafka
</li></ul></code></pre>
<p>Now that you&rsquo;ve started and enabled the services, it&rsquo;s time to check the installation.</p>

<h2 id="step-5-—-testing-the-installation">Step 5 — Testing the Installation</h2>

<p>Let&rsquo;s publish and consume a <code>Hello World</code> message to make sure the Kafka server is behaving correctly. Publishing messages in Kafka requires:</p>

<ul>
<li>A <em>producer</em>, which enables the publication of records and data to topics.</li>
<li>A <em>consumer</em>, which reads messages and data from topics.</li>
</ul>

<p>First, create a topic named <code>TutorialTopic</code> by typing:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">~/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic TutorialTopic
</li></ul></code></pre>
<p>You can create a producer from the command line using the <code>kafka-console-producer.sh</code> script. It expects the Kafka server&rsquo;s hostname, port, and a topic name as arguments.</p>

<p>Publish the string <code>Hello, World</code> to the <code>TutorialTopic</code> topic by typing:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo "Hello, World" | ~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic TutorialTopic &gt; /dev/null
</li></ul></code></pre>
<p>The <code>--broker-list</code> flag determines the list of message brokers to send the message to, in this case <code>localhost:9092</code>. <code>--topic</code> designates the topic as <code>TutorialTopic</code>.</p>

<p>Next, you can create a Kafka consumer using the <code>kafka-console-consumer.sh</code> script. It expects the ZooKeeper server&rsquo;s hostname and port and a topic name as arguments.</p>

<p>The following command consumes messages from <code>TutorialTopic</code>. Note the use of the <code>--from-beginning</code> flag, which allows the consumption of messages that were published before the consumer was started:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">~/kafka/bin/kafka-console-consumer.sh --bootstrap-server `localhost:9092` --topic TutorialTopic --from-beginning
</li></ul></code></pre>
<p><code>--bootstrap-server</code> provides a list of ingresses into the Kafka cluster. In this case, you are using <code>localhost:9092</code>.</p>

<p>You will see <code>Hello, World</code> in your terminal:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Hello, World
</code></pre>
<p>The script will continue to run, waiting for more messages to be published to the topic. Feel free to open a new terminal and start a producer to publish a few more messages. You should be able to see them all in the consumer&rsquo;s output. If you&rsquo;d like to learn more about how to use Kafka, see the <a href="https://kafka.apache.org/documentation/#gettingStarted">official Kafka documentation</a>.</p>

<p>When you are done testing, press <code>CTRL+C</code> to stop the consumer script. Now that you have tested the installation, you can move on to installing KafkaT in order to better administer your Kafka cluster.</p>

<h2 id="step-6-—-installing-kafkat-optional">Step 6 — Installing KafkaT (Optional)</h2>

<p><a href="https://github.com/airbnb/kafkat">KafkaT</a> is a tool from Airbnb that makes it easier for you to view details about your Kafka cluster and perform certain administrative tasks from the command line. Because it is a Ruby gem, you will need <a href="https://www.ruby-lang.org/en/">Ruby</a> to use it. You will also need the <code>build-essential</code> package to be able to build the other gems it depends on. Install them using <code>apt</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo apt install ruby ruby-dev build-essential
</li></ul></code></pre>
<p>You can now install KafkaT using the <code>gem</code> command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo CFLAGS=-Wno-error=format-overflow gem install kafkat
</li></ul></code></pre>
<p>The <code>CFLAGS=-Wno-error=format-overflow</code> option disables format overflow warnings and is required for the ZooKeeper gem, which is a dependency of KafkaT.</p>

<p>KafkaT uses <code>.kafkatcfg</code> as the configuration file to determine the installation and log directories of your Kafka server. It should also have an entry pointing KafkaT to your ZooKeeper instance.</p>

<p>Create a new file called <code>.kafkatcfg</code>:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">nano ~/.kafkatcfg
</li></ul></code></pre>
<p>Add the following lines to specify the required information about your Kafka server and Zookeeper instance:</p>
<div class="code-label " title="~/.kafkatcfg">~/.kafkatcfg</div><pre class="code-pre "><code langs="">{
  "kafka_path": "~/kafka",
  "log_path": "/tmp/kafka-logs",
  "zk_path": "localhost:2181"
}
</code></pre>
<p>You are now ready to use KafkaT. For a start, here&rsquo;s how you would use it to view details about all Kafka partitions:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">kafkat partitions
</li></ul></code></pre>
<p>You will see the following output:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Topic                 Partition   Leader      Replicas        ISRs    
TutorialTopic         0             0         [0]             [0]
__consumer_offsets    0             0         [0]             [0]
...
</code></pre>
<p>This output shows <code>TutorialTopic</code>, as well as <code>__consumer_offsets</code>, an internal topic used by Kafka for storing client-related information. You can safely ignore lines starting with <code>__consumer_offsets</code>.</p>

<p>To learn more about KafkaT, refer to its <a href="https://github.com/airbnb/kafkat">GitHub repository</a>.</p>

<p>Now that you have installed KafkaT, you can optionally set up Kafka on a cluster of Debian 10 servers to make a multi-node cluster.</p>

<h2 id="step-7-—-setting-up-a-multi-node-cluster-optional">Step 7 — Setting Up a Multi-Node Cluster (Optional)</h2>

<p>If you want to create a multi-broker cluster using more Debian 10 servers, repeat Step 1, Step 4, and Step 5 on each of the new machines. Additionally, make the following changes in the <code>~/kafka/config/server.properties</code> file for each:</p>

<ul>
<li><p>Change the value of the <code>broker.id</code> property such that it is unique throughout the cluster. This property uniquely identifies each server in the cluster and can have any string as its value. For example, <code>"server1"</code>, <code>"server2"</code>, etc., would be useful as identifiers.</p></li>
<li><p>Change the value of the <code>zookeeper.connect</code> property such that all nodes point to the same ZooKeeper instance. This property specifies the ZooKeeper instance&rsquo;s address and follows the <code>&lt;HOSTNAME/IP_ADDRESS&gt;:&lt;PORT&gt;</code> format. For this tutorial, you would use <code><span class="highlight">your_first_server_IP</span>:2181</code>, replacing <code><span class="highlight">your_first_server_IP</span></code> with the IP address of the Debian 10 server you already set up. </p></li>
</ul>

<p>If you want to have multiple ZooKeeper instances for your cluster, the value of the <code>zookeeper.connect</code> property on each node should be an identical, comma-separated string listing the IP addresses and port numbers of all the ZooKeeper instances.</p>

<p><span class='note'><strong>Note:</strong> If you have a firewall activated on the Debian 10 server with Zookeeper installed, make sure to open up port <code>2181</code> to allow for incoming requests from the other nodes in the cluster.<br></span></p>

<h2 id="step-8-—-restricting-the-kafka-user">Step 8 — Restricting the Kafka User</h2>

<p>Now that all of the installations are done, you can remove the <code>kafka</code> user&rsquo;s admin privileges. Before you do so, log out and log back in as any other non-root sudo user. If you are still running the same shell session you started this tutorial with, simply type <code>exit</code>.</p>

<p>Remove the <code>kafka</code> user from the sudo group:  </p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo deluser kafka sudo
</li></ul></code></pre>
<p>To further improve your Kafka server&rsquo;s security, lock the <code>kafka</code> user&rsquo;s password using the <code>passwd</code> command. This makes sure that nobody can directly log into the server using this account:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka -l
</li></ul></code></pre>
<p>At this point, only root or a sudo user can log in as <code>kafka</code> by typing in the following command:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo su - kafka
</li></ul></code></pre>
<p>In the future, if you want to unlock it, use <code>passwd</code> with the <code>-u</code> option:</p>
<pre class="code-pre command"><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo passwd kafka -u
</li></ul></code></pre>
<p>You have now successfully restricted the <code>kafka</code> user&rsquo;s admin privileges.</p>

<h2 id="conclusion">Conclusion</h2>

<p>You now have Apache Kafka running securely on your Debian server. You can make use of it in your projects by creating Kafka producers and consumers using <a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">Kafka clients</a>, which are available for most programming languages. To learn more about Kafka, you can also consult the <a href="http://kafka.apache.org/documentation.html">Apache Kafka documentation</a>.</p>
